# Awesome-Trojan-Attack-in-AI [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated, but probably biased and incomplete, list of awesome Trojan Attack in AI resources.

If you want to contribute to this list, feel free to pull a request. Also you can contact [Ruixiang Tang](https://scholar.google.com/citations?user=t4OwrwEAAAAJ&hl=en) from the [Data Lab](http://faculty.cs.tamu.edu/xiahu/) at Texas A&M University through email: rxtang@tamu.edu, or Twitter [@Ruixiang Tang](https://twitter.com/RuixiangT).


## What is Trojan Attack in AI?
With the widespread use of deep neural networks (DNNs) in highstake applications, the security problem of the DNN models has
received extensive attention. **Trojan attack** aims to attack deployed DNN systems relying on the hidden trigger patterns inserted by malicious developers or hackers. 

Before the final model packaging, malicious developers or hackers intentionally insert trojans into DNNs. During the inference
phase, an infected model with injected trojan performs normally on original tasks while behaves incorrectly with inputs stamped with
special triggers. 

## Trojan Attack
[An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks](https://arxiv.org/abs/2006.08131)
[BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)
[Trojaning Attack on Neural Networks](https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2782&context=cstech)
[Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning](https://arxiv.org/abs/1712.05526)
[Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](https://arxiv.org/abs/1804.00792)
[Model-Reuse Attacks on Deep Learning Systems](https://arxiv.org/abs/1812.00483)
[How To Backdoor Federated Learning](https://arxiv.org/abs/1807.00459)
[Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation](https://arxiv.org/abs/1808.10307)
[Backdooring Convolutional Neural Networks via TargetedWeight Perturbations](https://arxiv.org/abs/1812.03128)
[Latent Backdoor Attacks on Deep Neural Networks](https://arxiv.org/abs/1905.10447)
[Neural Trojans](https://arxiv.org/abs/1710.00942)

## Trojan Defense
[Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks](http://people.cs.uchicago.edu/~ravenben/publications/abstracts/backdoor-sp19.html)
[Detecting Backdoor Attacks on Deep Neural Networks by
Activation Clustering](https://arxiv.org/abs/1811.03728)
[Fine-Pruning: Defending Against Backdooring Attacks
on Deep Neural Networks](https://arxiv.org/abs/1805.12185)
[STRIP: A Defence Against Trojan Attacks on Deep Neural Networks](https://arxiv.org/abs/1902.06531)
[Spectral Signatures in Backdoor Attacks](https://arxiv.org/abs/1811.00636)
[DeepInspect: A Black-box Trojan Detection and Mitigation Framework for
Deep Neural Networks](https://www.ijcai.org/Proceedings/2019/647)
[ABS: Scanning Neural Networks for Back-doors by
Artificial Brain Stimulation](https://www.cs.purdue.edu/homes/taog/docs/CCS19.pdf)
[TABOR: A Highly Accurate Approach to Inspecting and
Restoring Trojan Backdoors in AI Systems](https://arxiv.org/abs/1908.01763)
[Defending Neural Backdoors via Generative
Distribution Modeling](https://arxiv.org/abs/1910.04749)

## Competition
[Trojans in Artificial Intelligence (TrojAI)](https://www.iarpa.gov/index.php/research-programs/trojai)







